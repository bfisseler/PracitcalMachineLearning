---
title: "Practical Machine Learning - Course Project"
author: "Björn Fisseler"
date: "April 2015"
output: html_document
---

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

# Basic strategy

- import and clean the data
- split the data for cross-validation and out-of-sample error estimation
- build three prediction models using different classification algorithms with training data
- use each prediction model to predict the test data
- compare the three models using Accuracy as the out-of-sample-error measure
- predict the 20 test cases

# Data Import + Preparation

Data is loaded from the internet. As the data contains several variables with missing data or non-data, only the columns with no NA are kept. We also drop the first seven variables, as these contain timestamps and user information, which are not relevant to the actual machine learning.

```{r, warning=FALSE, message=FALSE, cache=TRUE}
library(RCurl)
library(foreach)
#library(dplyr)
library(caret)

set.seed(1234)
# load the data
trainingData <- read.csv(text=getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv" , .opts = list(ssl.verifypeer = FALSE)), na.strings=c("NA","#DIV/0!", ""), stringsAsFactors = FALSE)

trainingData <- trainingData[,-c(1:7)]
trainingData <- trainingData[,colSums(is.na(trainingData)) == 0]
t <- nearZeroVar(trainingData, saveMetrics = TRUE)
trainingData <- trainingData[, t$nzv == FALSE]
trainingData$classe <- factor(trainingData$classe)

testingData <- read.csv(text=getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv" , .opts = list(ssl.verifypeer = FALSE)), na.strings=c("NA","#DIV/0!", ""), stringsAsFactors = FALSE)

testingData <- testingData[,-c(1:7)]
testingData <- testingData[,colSums(is.na(testingData)) == 0]
t <- nearZeroVar(testingData, saveMetrics = TRUE)
testingData <- testingData[, t$nzv == FALSE]
```

# Example of feature plot

We just do a small feature plot for fun.

```{r, warning=FALSE, message=FALSE, cache=TRUE}
featurePlot(x = trainingData[, c(1:3)], y = trainingData$classe, plot = "pairs")
```

# Split the data for cross validation

We split the data into training and testing data (60%/40%). Additionally each model is build using five-fold cross-validation.

```{r, warning=FALSE, message=FALSE, cache=TRUE}
inTrain <- createDataPartition(trainingData$classe, p=.60, list=FALSE)

trainTrain <- trainingData[inTrain,]
testTrain <- trainingData[-inTrain,]
```

# Build first model: Decision Tree with J48

First model is a decision tree, build using J48.

```{r, warning=FALSE, message=FALSE, cache=TRUE}
modJ48 <- train(classe ~ ., data = trainTrain, method = 'J48', trControl=trainControl(method="cv", number=5, repeats=2))
print(modJ48)
pred.J48 <- predict(modJ48, testTrain)
cm.J48 <- confusionMatrix(pred.J48, testTrain$classe)
print(cm.J48)
```

# Build second model: Classification Trees

The second model is a classification tree.

```{r, warning=FALSE, message=FALSE, cache=TRUE}
modRPart <- train(classe ~ ., data = trainTrain, method = 'rpart', trControl=trainControl(method="cv", number=5, repeats=2))
print(modRPart)
plot(modRPart$finalModel, uniform=TRUE, main = "Classification Tree")
text(modRPart$finalModel, use.n=TRUE, all=TRUE, cex=.8)
pred.RPart <- predict(modRPart, testTrain)
cm.RPart <- confusionMatrix(pred.RPart, testTrain$classe)
print(cm.RPart)
```

# Build third model: Random Forest

And Random Forest is used for the third model. As RF takes quite some time to compute, the execution is parallelized.

```{r, warning=FALSE, message=FALSE, cache=TRUE}
#library(doMC)
#registerDoMC(cores = 3) #speed up things on multicore
library(doParallel)
cl <- makeCluster(2) #(detectCores())
registerDoParallel(cl)

modRF <- train(classe ~ ., data = trainTrain, method = 'rf', trControl=trainControl(method="cv", number=5, repeats=2), prox=TRUE,allowParallel=TRUE)
stopCluster(cl)
stopImplicitCluster()
registerDoSEQ()
gc()
plot(modRF$finalModel)
pred.RF <- predict(modRF, testTrain)
cm.RF <- confusionMatrix(pred.RF, testTrain$classe)
print(cm.RF)
detach("package:doParallel", unload=TRUE)
```

# Model Selection based on Out-of-Sample-Error

We use *Accuracy* as the the OoS-Error-Measure. Accuracy is defined as "the proportion of true results (both true positives and true negatives) among the total number of cases examined." (Wikipedia, URL: https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification)

- Decision Tree: 0.9508
- Classification Tree: 0.4904
- Random Forest: 0.9912

# Predict

The best ML model is build using Random Forest (Accurary: 0.99, Kappa: 0.99). Therefor the prediction of the testing data is done using the model build with RF.

```{r, warning=FALSE, message=FALSE, cache=TRUE}
pred.testing <- predict(modRF, testingData)
print(pred.testing)
```